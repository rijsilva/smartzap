/**
 * Chat Agent - Tool-based RAG (Vercel AI SDK pattern)
 *
 * Agente de chat que processa conversas do inbox usando IA.
 * Suporta m√∫ltiplos providers: Google (Gemini), OpenAI (GPT), Anthropic (Claude).
 *
 * Usa RAG pr√≥prio com Supabase pgvector seguindo o padr√£o recomendado pela Vercel:
 * - O LLM recebe uma tool `searchKnowledgeBase` e DECIDE quando us√°-la
 * - Para sauda√ß√µes ("oie") ‚Üí responde direto, sem buscar
 * - Para perguntas ("qual o hor√°rio?") ‚Üí chama a tool, depois responde
 *
 * Isso √© mais eficiente que "eager RAG" (sempre buscar) porque:
 * - Reduz lat√™ncia em mensagens que n√£o precisam de contexto
 * - Reduz custos de embedding (menos queries)
 * - Evita injetar ru√≠do em conversas simples
 */

import { z } from 'zod'
import { getSupabaseAdmin } from '@/lib/supabase'
import type { AIAgent, InboxConversation, InboxMessage } from '@/types'

// NOTE: AI dependencies are imported DYNAMICALLY inside processChatAgent
// This is required because static imports can cause issues when called from
// background contexts (like debounced webhook handlers)

// =============================================================================
// Debounce Manager
// =============================================================================

/**
 * Track pending responses to implement debounce
 * Key: conversationId, Value: timeout handle and last message timestamp
 */
const pendingResponses = new Map<
  string,
  {
    timeout: NodeJS.Timeout
    lastMessageAt: number
    messageIds: string[]
  }
>()

/**
 * Check if we should wait for more messages (debounce)
 * Returns true if we should delay processing
 */
export function shouldDebounce(
  conversationId: string,
  debounceSec: number = 5
): boolean {
  const pending = pendingResponses.get(conversationId)
  if (!pending) return false

  const elapsed = Date.now() - pending.lastMessageAt
  return elapsed < debounceSec * 1000
}

/**
 * Schedule agent processing with debounce
 * Returns a promise that resolves when processing should begin
 */
export function scheduleWithDebounce(
  conversationId: string,
  messageId: string,
  debounceSec: number = 5
): Promise<string[]> {
  return new Promise((resolve) => {
    const pending = pendingResponses.get(conversationId)

    // Clear existing timeout
    if (pending?.timeout) {
      clearTimeout(pending.timeout)
    }

    // Accumulate message IDs
    const messageIds = pending?.messageIds || []
    messageIds.push(messageId)

    // Set new timeout
    const timeout = setTimeout(() => {
      const accumulated = pendingResponses.get(conversationId)
      pendingResponses.delete(conversationId)
      resolve(accumulated?.messageIds || messageIds)
    }, debounceSec * 1000)

    pendingResponses.set(conversationId, {
      timeout,
      lastMessageAt: Date.now(),
      messageIds,
    })
  })
}

/**
 * Cancel pending debounce for a conversation
 */
export function cancelDebounce(conversationId: string): void {
  const pending = pendingResponses.get(conversationId)
  if (pending?.timeout) {
    clearTimeout(pending.timeout)
    pendingResponses.delete(conversationId)
  }
}

// =============================================================================
// Types
// =============================================================================

export interface SupportAgentConfig {
  agent: AIAgent
  conversation: InboxConversation
  messages: InboxMessage[]
}

export interface SupportAgentResult {
  success: boolean
  response?: SupportResponse
  error?: string
  latencyMs: number
  logId?: string
}

// =============================================================================
// Response Schema
// =============================================================================

// Schema base (sem handoff)
const baseResponseSchema = z.object({
  message: z.string().describe('A resposta para enviar ao usu√°rio'),
  sentiment: z
    .enum(['positive', 'neutral', 'negative', 'frustrated'])
    .describe('Sentimento detectado na mensagem do usu√°rio'),
  confidence: z
    .number()
    .min(0)
    .max(1)
    .describe('N√≠vel de confian√ßa na resposta (0 = incerto, 1 = certo)'),
  sources: z
    .array(
      z.object({
        title: z.string(),
        content: z.string(),
      })
    )
    .optional()
    .describe('Fontes utilizadas para gerar a resposta'),
})

// Campos de handoff (adicionados quando habilitado)
const handoffFields = {
  shouldHandoff: z
    .boolean()
    .describe('Se deve transferir para um atendente humano'),
  handoffReason: z
    .string()
    .optional()
    .describe('Motivo da transfer√™ncia para humano'),
  handoffSummary: z
    .string()
    .optional()
    .describe('Resumo da conversa para o atendente'),
}

// Schema completo (com handoff) - mantido para compatibilidade
const supportResponseSchema = baseResponseSchema.extend(handoffFields)

/**
 * Gera o schema de resposta baseado na configura√ß√£o do agente
 */
function getResponseSchema(handoffEnabled: boolean) {
  if (handoffEnabled) {
    return baseResponseSchema.extend(handoffFields)
  }
  return baseResponseSchema
}

export type SupportResponse = z.infer<typeof supportResponseSchema>

// =============================================================================
// Constants
// =============================================================================

const DEFAULT_MODEL_ID = 'gemini-3-flash-preview'
const DEFAULT_TEMPERATURE = 0.7
const DEFAULT_MAX_TOKENS = 2048

/**
 * Converte formata√ß√£o Markdown para WhatsApp.
 * Executado ap√≥s a resposta do LLM (zero tokens extras).
 *
 * Markdown ‚Üí WhatsApp:
 * - **texto** ‚Üí *texto* (negrito)
 * - __texto__ ‚Üí *texto* (negrito alternativo)
 * - ~~texto~~ ‚Üí ~texto~ (riscado)
 * - [texto](url) ‚Üí texto (url) ou s√≥ url se forem iguais
 */
function convertMarkdownToWhatsApp(text: string): string {
  return text
    // **texto** ou __texto__ ‚Üí *texto* (negrito)
    .replace(/\*\*(.+?)\*\*/g, '*$1*')
    .replace(/__(.+?)__/g, '*$1*')
    // ~~texto~~ ‚Üí ~texto~ (riscado)
    .replace(/~~(.+?)~~/g, '~$1~')
    // [texto](url) ‚Üí converte links Markdown
    .replace(/\[([^\]]+)\]\(([^)]+)\)/g, (_, linkText, url) => {
      // Se o texto √© basicamente a URL (com ou sem protocolo), s√≥ retorna a URL
      const cleanText = linkText.replace(/^https?:\/\//, '').replace(/\/$/, '')
      const cleanUrl = url.replace(/^https?:\/\//, '').replace(/\/$/, '')
      if (cleanText === cleanUrl || linkText === url) {
        return url
      }
      // Sen√£o, retorna "texto: url"
      return `${linkText}: ${url}`
    })
}

// =============================================================================
// Helpers
// =============================================================================

function convertToAIMessages(
  messages: InboxMessage[]
): Array<{ role: 'user' | 'assistant'; content: string }> {
  return messages
    .filter((m) => m.message_type !== 'internal_note')
    .map((m) => ({
      role: (m.direction === 'inbound' ? 'user' : 'assistant') as 'user' | 'assistant',
      content: m.content,
    }))
}


async function persistAILog(data: {
  conversationId: string
  agentId: string
  messageIds: string[]
  input: string
  output: SupportResponse | null
  latencyMs: number
  error: string | null
  modelUsed: string
}): Promise<string | undefined> {
  try {
    const supabase = getSupabaseAdmin()
    if (!supabase) {
      console.error('[chat-agent] Supabase admin client not available')
      return undefined
    }
    const { data: log, error } = await supabase
      .from('ai_agent_logs')
      .insert({
        conversation_id: data.conversationId,
        ai_agent_id: data.agentId,
        input_message: data.input,
        output_message: data.output?.message || null,
        response_time_ms: data.latencyMs,
        model_used: data.modelUsed,
        tokens_used: null,
        sources_used: data.output?.sources || null,
        error_message: data.error,
        metadata: {
          messageIds: data.messageIds,
          sentiment: data.output?.sentiment,
          confidence: data.output?.confidence,
          shouldHandoff: data.output?.shouldHandoff,
          handoffReason: data.output?.handoffReason,
        },
      })
      .select('id')
      .single()

    if (error) {
      console.error('[chat-agent] Failed to persist log:', error)
      return undefined
    }
    return log?.id
  } catch (err) {
    console.error('[chat-agent] Log error:', err)
    return undefined
  }
}

// =============================================================================
// Main Function
// =============================================================================

export async function processChatAgent(
  config: SupportAgentConfig
): Promise<SupportAgentResult> {
  const { agent, conversation, messages } = config
  const startTime = Date.now()

  // Dynamic imports - required for background execution context
  const { generateText, tool, stepCountIs } = await import('ai')
  const { withDevTools } = await import('@/lib/ai/devtools')
  const { createLanguageModel, getProviderFromModel } = await import('@/lib/ai/provider-factory')
  const {
    findRelevantContent,
    hasIndexedContent,
    buildEmbeddingConfigFromAgent,
    buildRerankConfigFromAgent,
  } = await import('@/lib/ai/rag-store')

  // Setup message context
  const lastUserMessage = messages.filter((m) => m.direction === 'inbound').slice(-1)[0]
  const inputText = lastUserMessage?.content || ''
  const messageIds = messages.map((m) => m.id)
  const aiMessages = convertToAIMessages(messages.slice(-10))

  // =======================================================================
  // MEM0: Fetch relevant memories (graceful degradation)
  // =======================================================================
  const { fetchRelevantMemories, saveInteractionMemory, isMem0EnabledAsync } = await import('@/lib/ai/mem0-client')

  let memoryContext = { systemPromptAddition: '', memoryCount: 0 }
  const mem0Enabled = await isMem0EnabledAsync()
  if (mem0Enabled) {
    console.log(`[chat-agent] Mem0 enabled, fetching memories for ${conversation.phone}...`)
    memoryContext = await fetchRelevantMemories(inputText, {
      user_id: conversation.phone,
      agent_id: agent.id,
    })
    if (memoryContext.memoryCount > 0) {
      console.log(`[chat-agent] Found ${memoryContext.memoryCount} memories`)
    }
  }

  // Get model configuration - supports Google, OpenAI, Anthropic
  const modelId = agent.model || DEFAULT_MODEL_ID
  const provider = getProviderFromModel(modelId)

  let baseModel
  let apiKey: string
  try {
    const result = await createLanguageModel(modelId)
    baseModel = result.model
    apiKey = result.apiKey
  } catch (err) {
    return {
      success: false,
      error: err instanceof Error ? err.message : 'Erro ao criar modelo de IA',
      latencyMs: Date.now() - startTime,
    }
  }

  const model = await withDevTools(baseModel, { name: `agente:${agent.name}` })

  console.log(`[chat-agent] Using provider: ${provider}, model: ${modelId}`)

  // Check if agent has indexed content in pgvector
  const hasKnowledgeBase = await hasIndexedContent(agent.id)

  console.log(`[chat-agent] Processing: model=${modelId}, hasKnowledgeBase=${hasKnowledgeBase}`)
  console.log(`[chat-agent] Total messages received: ${messages.length}`)
  console.log(`[chat-agent] Last user message: "${inputText.slice(0, 100)}..."`)

  let response: SupportResponse | undefined
  let error: string | null = null
  let sources: Array<{ title: string; content: string }> | undefined

  try {
    // =======================================================================
    // TOOL-BASED RAG: LLM decides when to search
    // =======================================================================

    // Use agent's system prompt + memory context (if available)
    const systemPrompt = memoryContext.systemPromptAddition
      ? `${agent.system_prompt}\n\n${memoryContext.systemPromptAddition}`
      : agent.system_prompt

    // Define respond tool (required for structured output)
    // Schema √© din√¢mico baseado em handoff_enabled
    const handoffEnabled = agent.handoff_enabled ?? true // default true para compatibilidade
    const responseSchema = getResponseSchema(handoffEnabled)

    console.log(`[chat-agent] Handoff enabled: ${handoffEnabled}`)

    // Flag para indicar que j√° respondeu (para stopWhen)
    let hasResponded = false

    const respondTool = tool({
      description: 'Envia uma resposta estruturada ao usu√°rio. Use APENAS quando tiver a resposta final. N√ÉO use para respostas parciais.',
      inputSchema: responseSchema,
      execute: async (params) => {
        const handoffParams = params as { shouldHandoff?: boolean }
        // Converte Markdown ‚Üí WhatsApp (zero tokens extras, s√≥ post-processing)
        const formattedMessage = convertMarkdownToWhatsApp(params.message)
        response = {
          ...params,
          message: formattedMessage,
          shouldHandoff: handoffParams.shouldHandoff ?? false,
          sources: sources || params.sources,
        }
        hasResponded = true // Marca que j√° respondeu
        return { success: true, message: formattedMessage }
      },
    })

    // Knowledge base search tool - only created if agent has indexed content
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    let searchKnowledgeBaseTool: any = undefined

    if (hasKnowledgeBase) {
      searchKnowledgeBaseTool = tool({
        description: 'Busca informa√ß√µes na base de conhecimento do agente. Use para responder perguntas que precisam de dados espec√≠ficos.',
        inputSchema: z.object({
          query: z.string().describe('A pergunta ou termos de busca para encontrar informa√ß√µes relevantes'),
        }),
        execute: async ({ query }) => {
          console.log(`[chat-agent] LLM requested knowledge search: "${query.slice(0, 100)}..."`)
          const ragStartTime = Date.now()

          // Build configs
          const embeddingConfig = buildEmbeddingConfigFromAgent(agent, apiKey)
          const rerankConfig = await buildRerankConfigFromAgent(agent)

          // Search
          const relevantContent = await findRelevantContent({
            agentId: agent.id,
            query,
            embeddingConfig,
            rerankConfig,
            topK: agent.rag_max_results || 5,
            threshold: agent.rag_similarity_threshold || 0.5,
          })

          console.log(`[chat-agent] RAG search completed in ${Date.now() - ragStartTime}ms, found ${relevantContent.length} chunks`)

          if (relevantContent.length === 0) {
            return { found: false, message: 'Nenhuma informa√ß√£o relevante encontrada na base de conhecimento.' }
          }

          // Track sources for logging
          sources = relevantContent.map((r, i) => ({
            title: `Fonte ${i + 1}`,
            content: r.content.slice(0, 200) + '...',
          }))

          // Return formatted content for LLM to use
          const contextText = relevantContent
            .map((r, i) => `[${i + 1}] ${r.content}`)
            .join('\n\n')

          return {
            found: true,
            content: contextText,
            sourceCount: relevantContent.length,
          }
        },
      })
    }

    // Build tools object
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    const tools: Record<string, any> = { respond: respondTool }
    if (searchKnowledgeBaseTool) {
      tools.searchKnowledgeBase = searchKnowledgeBaseTool
    }

    // Booking Flow tool - only created if agent has booking tool enabled
    if (agent.booking_tool_enabled) {
      console.log(`[chat-agent] üìÖ Booking tool is enabled, checking prerequisites...`)
      const { sendBookingFlow, checkBookingPrerequisites, BOOKING_TOOL_DESCRIPTION } = await import('@/lib/ai/tools/booking-tool')

      // Check if prerequisites are met (async check)
      const prereqs = await checkBookingPrerequisites()
      console.log(`[chat-agent] üìÖ Prerequisites check: ready=${prereqs.ready}, missing=${prereqs.missing.join(', ') || 'none'}`)

      if (prereqs.ready) {
        const sendBookingFlowTool = tool({
          description: BOOKING_TOOL_DESCRIPTION,
          // Schema com campo opcional - alguns providers n√£o lidam bem com schemas vazios
          inputSchema: z.object({
            confirm: z.boolean().optional().describe('Confirma√ß√£o para enviar o formul√°rio de agendamento (sempre true)')
          }),
          execute: async () => {
            console.log(`[chat-agent] üìÖ LLM requested booking flow for: ${conversation.phone}`)
            const result = await sendBookingFlow(conversation.phone)

            if (result.success) {
              console.log(`[chat-agent] üìÖ Booking flow sent successfully: ${result.messageId}`)
              return {
                sent: true,
                message: 'Formul√°rio de agendamento enviado com sucesso. O cliente ver√° os hor√°rios dispon√≠veis.',
              }
            }
            console.log(`[chat-agent] üìÖ Failed to send booking flow: ${result.error}`)
            return {
              sent: false,
              message: `N√£o foi poss√≠vel enviar o formul√°rio: ${result.error}`,
            }
          },
        })
        tools.sendBookingFlow = sendBookingFlowTool
        console.log(`[chat-agent] üìÖ Booking tool added to tools list`)
      } else {
        console.log(`[chat-agent] ‚ö†Ô∏è Booking tool enabled but prerequisites not met: ${prereqs.missing.join(', ')}`)
      }
    }

    // Determina se precisa de multi-step (mais de uma tool al√©m de respond)
    const hasMultipleTools = Object.keys(tools).length > 1
    console.log(`[chat-agent] Generating response with tools: ${Object.keys(tools).join(', ')}, multiStep: ${hasMultipleTools}`)

    // Generate with multi-step support when we have multiple tools
    // Condi√ß√£o de parada: para assim que respond for chamado OU ap√≥s 3 steps
    const stopCondition = () => {
      if (hasResponded) {
        console.log(`[chat-agent] üõë Stopping: respond tool was called`)
        return true
      }
      return false
    }

    console.log(`[chat-agent] üöÄ Calling generateText...`)
    const startGenerate = Date.now()

    try {
      const result = await generateText({
        model,
        system: systemPrompt,
        messages: aiMessages,
        tools,
        // Para quando respond for chamado OU ap√≥s 3 steps (o que vier primeiro)
        stopWhen: (event) => stopCondition() || stepCountIs(3)(event),
        temperature: agent.temperature ?? DEFAULT_TEMPERATURE,
        maxOutputTokens: agent.max_tokens ?? DEFAULT_MAX_TOKENS,
      })

      console.log(`[chat-agent] ‚úÖ generateText completed in ${Date.now() - startGenerate}ms`)
      console.log(`[chat-agent] Steps executed: ${result.steps?.length || 0}`)
      console.log(`[chat-agent] Tool calls: ${JSON.stringify(result.steps?.map(s => s.toolCalls?.map(tc => tc.toolName)).flat().filter(Boolean) || [])}`)
      console.log(`[chat-agent] Finish reason: ${result.finishReason}`)

      // Log each step for debugging
      result.steps?.forEach((step, i) => {
        console.log(`[chat-agent] Step ${i + 1}: toolCalls=${step.toolCalls?.map(tc => tc.toolName).join(', ') || 'none'}, text=${step.text?.slice(0, 50) || 'none'}...`)
      })

    } catch (genError) {
      console.error(`[chat-agent] ‚ùå generateText failed after ${Date.now() - startGenerate}ms:`, genError)
      throw genError
    }

    if (!response) {
      console.error(`[chat-agent] ‚ö†Ô∏è No response object - respond tool was not called`)
      throw new Error('No response generated - LLM did not call respond tool')
    }

    console.log(`[chat-agent] Response generated: "${response.message.slice(0, 100)}..."`)
    if (sources) {
      console.log(`[chat-agent] Used ${sources.length} knowledge base sources`)
    } else {
      console.log(`[chat-agent] No knowledge base search performed`)
    }

  } catch (err) {
    error = err instanceof Error ? err.message : 'Unknown error'
    console.error('[chat-agent] Error:', error)
  }

  const latencyMs = Date.now() - startTime

  // Success case
  if (response) {
    const logId = await persistAILog({
      conversationId: conversation.id,
      agentId: agent.id,
      messageIds,
      input: inputText,
      output: response,
      latencyMs,
      error: null,
      modelUsed: modelId,
    })

    // Save interaction to Mem0 (fire-and-forget, n√£o bloqueia resposta)
    if (mem0Enabled) {
      saveInteractionMemory(
        [
          { role: 'user', content: inputText },
          { role: 'assistant', content: response.message },
        ],
        {
          user_id: conversation.phone,
          agent_id: agent.id,
        }
      ).catch((err) => {
        console.warn(`[chat-agent] Failed to save memory: ${err.message}`)
      })
    }

    return { success: true, response, latencyMs, logId }
  }

  // Error case - auto handoff
  const handoffResponse: SupportResponse = {
    message: 'Desculpe, estou com dificuldades t√©cnicas. Vou transferir voc√™ para um atendente.',
    sentiment: 'neutral',
    confidence: 0,
    shouldHandoff: true,
    handoffReason: `Erro t√©cnico: ${error}`,
    handoffSummary: `Erro durante processamento. √öltima mensagem: "${inputText.slice(0, 200)}"`,
  }

  const logId = await persistAILog({
    conversationId: conversation.id,
    agentId: agent.id,
    messageIds,
    input: inputText,
    output: handoffResponse,
    latencyMs,
    error,
    modelUsed: modelId,
  })

  return {
    success: false,
    response: handoffResponse,
    error: error || 'Unknown error',
    latencyMs,
    logId,
  }
}
